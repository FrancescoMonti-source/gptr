% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gpt_chat.R
\name{gpt_chat}
\alias{gpt_chat}
\alias{$.gpt_chat_callable}
\title{Stateful chat with OpenAI-compatible servers (LM Studio, OpenAI, etc.)}
\usage{
gpt_chat(
  prompt = NULL,
  model = NULL,
  temperature = 0.2,
  provider = c("auto", "lmstudio", "openai", "ollama", "local", "localai"),
  base_url = NULL,
  openai_api_key = Sys.getenv("OPENAI_API_KEY", ""),
  save_path = NULL,
  load_path = NULL,
  trim_max_chars = NULL,
  ...
)

gpt_chat$<method>
}
\arguments{
\item{prompt}{Character scalar, the new user input. Required unless showing
or resetting history.}

\item{model}{Character scalar. Model identifier to use (provider-specific).}

\item{temperature}{Numeric. Sampling temperature (default 0.2).}

\item{provider}{One of \code{"auto"}, \code{"lmstudio"}, \code{"openai"}, \code{"ollama"},
\code{"local"}, \code{"localai"}. Default \code{"auto"}.}

\item{base_url}{Override base URL for the provider (optional).}

\item{openai_api_key}{API key for OpenAI (default reads from env var).}

\item{save_path}{File path. If given, conversation history is saved after
each turn as pretty-printed JSON.}

\item{load_path}{File path. If given, conversation history is loaded from
JSON on disk before proceeding.}

\item{trim_max_chars}{Integer. If given, trims history to fit within this
approximate character budget.}

\item{...}{Passed to \code{\link[=gpt]{gpt()}}.}
}
\value{
Invisibly returns the model reply (character scalar) for normal turns.
}
\description{
\code{gpt_chat()} provides a persistent, multi-turn chat interface that keeps
conversation history in memory and (optionally) on disk. It speaks the
OpenAI Chat Completions API, so it works with local servers like \strong{LM Studio}
as well as hosted providers such as \strong{OpenAI} (set \code{api_key}).
}
\details{
\strong{Available methods} (call with \code{gpt_chat$method(...)}):
\itemize{
\item \code{show_history(as_text = TRUE)} – Print (or return tibble when \code{FALSE}).
\item \code{reset()} – Clear the in-memory history.
\item \code{get_history()} – Return the raw list of messages.
\item \code{save(path)} / \code{load(path)} – Persist/restore history as JSON.
\item \code{replace_history(x)} – Overwrite history with \code{x} (list of messages).
\item \verb{summarise(model=..., provider=..., base_url=..., temperature=..., replace=TRUE, prefix=\\"Chat summary : \\")}
– Summarise the current history via \code{gpt()}; by default replaces history with the summary message.
}

\strong{Available methods} (call with \code{gpt_chat$method(...)}):
\itemize{
\item \code{show_history(as_text = TRUE)}
Print the conversation (or return a tibble when \code{as_text = FALSE}).
\item \code{reset()}
Clear the in-memory history.
\item \code{get_history()}
Return the raw list of message objects.
\item \code{save(path)} / \code{load(path)}
Persist or restore the history as JSON.
}
}
\section{What it does}{

\itemize{
\item Maintains \strong{conversation memory} across turns (system/user/assistant roles).
\item Accepts \strong{file} (\code{.txt/.md/.csv/.log}, \code{.pdf}, \code{.docx}) and \strong{image}
(base64 data URL) inputs alongside text prompts.
\item Can \strong{trim context} to a character budget for small-context models.
\item Can \strong{save}/ \strong{load} history to/from JSON for persistence.
}
}

\examples{
\dontrun{
# Normal usage (callable):
gpt_chat("Hello, who are you?")

# Methods:
gpt_chat$show_history()
gpt_chat$reset()

# Save/load:
gpt_chat$save("chat.json")
gpt_chat$load("chat.json")
gpt_chat$show_history()
}
}
