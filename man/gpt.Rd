% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gpt.R
\name{gpt}
\alias{gpt}
\title{Chat once with the selected provider}
\usage{
gpt(
  prompt,
  model = NULL,
  temperature = 0.2,
  provider = c("auto", "lmstudio", "openai", "ollama", "local", "localai"),
  base_url = NULL,
  openai_api_key = Sys.getenv("OPENAI_API_KEY", unset = ""),
  image_path = NULL,
  system = NULL,
  seed = NULL,
  response_format = NULL,
  backend = NULL,
  strict_model = getOption("gptr.strict_model", TRUE),
  print_raw = FALSE,
  ...
)
}
\arguments{
\item{prompt}{Character scalar user message. Ignored if \code{messages} is supplied upstream.}

\item{model}{Optional model id. If NULL, resolved per provider defaults.}

\item{temperature}{Numeric scalar (default 0.2).}

\item{provider}{One of "auto", "local", "openai", "lmstudio", "ollama", "localai".}

\item{base_url}{“Optional. Pin a specific local endpoint (…/v1 or …/v1/chat/completions).”}

\item{openai_api_key}{Optional API key for OpenAI; defaults to env var.}

\item{image_path}{Optional path or vector of paths to images to include.}

\item{system}{Optional system prompt.}

\item{seed}{Optional integer for determinism (when supported).}

\item{response_format}{NULL, "json_object", or a full list (OpenAI API shape).}

\item{backend}{“Optional. When provider is local, choose a running backend ('lmstudio', 'ollama', 'localai').”}

\item{print_raw}{Logical. If TRUE, pretty-print a compact response skeleton and return it immediately (skips any post-processing). Default FALSE.}

\item{...}{Extra fields passed through to the provider payload (e.g. \code{max_tokens}, \code{stop}).}
}
\value{
Character scalar (assistant message). \code{attr(value, "usage")} may contain token usage.
}
\description{
Minimal front-door that delegates to provider-specific helpers.
Returns plain text (character scalar). Usage is attached as an attribute.
}
