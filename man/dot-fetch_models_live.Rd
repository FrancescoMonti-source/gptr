% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/models_cache.R
\name{.fetch_models_live}
\alias{.fetch_models_live}
\title{Perform a live HTTP GET on /v1/models for a given provider and base_url.
Returns a list with a data frame of models (\code{df}) and a status string.
Branches on provider to apply OpenAI-specific headers and retry logic or
a generic flow for local backends.}
\usage{
.fetch_models_live(
  provider,
  base_url,
  refresh = FALSE,
  openai_api_key = Sys.getenv("OPENAI_API_KEY", ""),
  timeout = getOption("gptr.request_timeout", 5),
  ssl_cert = NULL
)
}
\arguments{
\item{provider}{Provider name (e.g., "openai", "ollama").}

\item{base_url}{Base URL of the backend.}

\item{refresh}{Logical flag indicating whether the caller wishes to bypass
cached results.  This function itself always performs a live request and
does not read from or write to the cache, but downstream callers use the
flag to signal a cache bypass.}

\item{openai_api_key}{OpenAI API key used when \code{provider = "openai"}.}

\item{timeout}{Request timeout in seconds.}

\item{ssl_cert}{Optional path to a certificate authority (CA) bundle passed to
\code{\link[httr2:req_options]{httr2::req_options()}} as \code{cainfo} for HTTPS verification.}
}
\description{
Perform a live HTTP GET on /v1/models for a given provider and base_url.
Returns a list with a data frame of models (\code{df}) and a status string.
Branches on provider to apply OpenAI-specific headers and retry logic or
a generic flow for local backends.
}
\keyword{internal}
